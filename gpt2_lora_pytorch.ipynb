{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DrjAmiETxxr"
   },
   "source": [
    "# Parameter-efficient fine-tuning of GPT-2 with LoRA (PyTorch)\n",
    "\n",
    "**Description:** Use PyTorch and PEFT to fine-tune a GPT-2 LLM with LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUDah9mYTxxs"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) have been shown to be effective at a variety of NLP\n",
    "tasks. An LLM is first pre-trained on a large corpus of text in a\n",
    "self-supervised fashion. Pre-training helps LLMs learn general-purpose knowledge,\n",
    "such as statistical relationships between words. An LLM can then be fine-tuned\n",
    "on a downstream task of interest (such as sentiment analysis).\n",
    "\n",
    "However, LLMs are extremely large in size, and we don't need to train all the\n",
    "parameters in the model while fine-tuning, especially because datasets on which\n",
    "the model is fine-tuned are relatively small. Another way of saying this is\n",
    "that LLMs are over-parametrized for fine-tuning. This is where\n",
    "[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685) comes in; it\n",
    "significantly reduces the number of trainable parameters. This results in a\n",
    "decrease in training time and GPU memory usage, while maintaining the quality\n",
    "of the outputs.\n",
    "\n",
    "In this example, we will explain LoRA in technical terms, show how the technical\n",
    "explanation translates to code, use PyTorch and Hugging Face's PEFT library to implement\n",
    "[GPT-2 model](https://huggingface.co/gpt2) and fine-tune\n",
    "it on the next token prediction task using LoRA. We will compare LoRA GPT-2\n",
    "with a fully fine-tuned GPT-2 in terms of the quality of the generated text,\n",
    "training time and GPU memory usage.\n",
    "\n",
    "Note: This example runs on PyTorch with CUDA support optimized for Linux systems\n",
    "with CUDA 12.x compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RbBiRktTxxs"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before we start implementing the pipeline, let's install and import all the\n",
    "libraries we need. We'll be using PyTorch, Transformers, and PEFT libraries.\n",
    "\n",
    "Secondly, let's enable mixed precision training. This will help us reduce the\n",
    "training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6QJI-XewTxxs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed hf-xet-1.1.8 huggingface-hub-0.34.4 regex-2025.7.34 safetensors-0.6.2 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.4\n",
      "Collecting peft\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (4.55.4)\n",
      "Requirement already satisfied: tqdm in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2025.7.0)\n",
      "Requirement already satisfied: requests in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.8)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers->peft) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from transformers->peft) (0.21.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.10.1 peft-0.17.1\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Using cached pandas-2.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Using cached xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Using cached multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Using cached propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Using cached yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.4 multiprocess-0.70.16 pandas-2.3.2 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "Requirement already satisfied: accelerate in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.8)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Requirement already satisfied: matplotlib in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages optimized for CUDA 12.x\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zjkyd6HWTxxt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA Version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Enable mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTD_dPMtTxxt"
   },
   "source": [
    "Let's also define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dOs9MTZnTxxt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Effective batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Memory-optimized hyperparameters\n",
    "BATCH_SIZE = 4  # Reduced from 32\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # This gives effective batch size of 32\n",
    "NUM_BATCHES = 50  # Reduced for memory\n",
    "EPOCHS = 10 # Can be set to a higher value for better results\n",
    "MAX_SEQUENCE_LENGTH = 64  # Reduced from 128\n",
    "MAX_GENERATION_LENGTH = 700\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "GPT2_MODEL_NAME = \"gpt2\"\n",
    "\n",
    "# LoRA-specific hyperparameters\n",
    "LORA_RANK = 4\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Set memory optimization FIRST\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'\n",
    "\n",
    "print(f\"✅ Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W8007pgTxxu"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load a Reddit dataset. We will fine-tune both the GPT-2 model and the\n",
    "LoRA GPT-2 model on a subset of this dataset. The aim is to produce text similar\n",
    "in style to Reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "F-c7yXqdTxxu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 619\n"
     ]
    }
   ],
   "source": [
    "# Load Reddit TIFU dataset\n",
    "try:\n",
    "    dataset = load_dataset(\"Fredithefish/Reddit-TIFU\", split=\"train\")\n",
    "    \n",
    "    # Map to expected 'documents' field if needed\n",
    "    if 'documents' not in dataset.column_names:\n",
    "        def map_to_documents(example):\n",
    "            return {'documents': example.get('selftext', example.get('title', ''))}\n",
    "        dataset = dataset.map(map_to_documents)\n",
    "        \n",
    "except:\n",
    "    # Fallback to custom dataset if download fails\n",
    "    from datasets import Dataset\n",
    "    texts = [\"TIFU by accidentally sending an embarrassing text to the wrong person.\"] * (NUM_BATCHES * BATCH_SIZE)\n",
    "    dataset = Dataset.from_dict({'documents': texts})\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtOROLo4Txxu"
   },
   "source": [
    "The dataset has two fields: `documents` and `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tm45e-LITxxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "TIFU by raising the flag upside down on a military base and causing local farmers to think the base was in distress....\n",
      "\n",
      "Sample title:\n",
      "TIFU by raising the flag upside down on a military base and causing local farmers to think the base was in distress.\n"
     ]
    }
   ],
   "source": [
    "# Examine dataset structure\n",
    "sample = dataset[0]\n",
    "print(\"Sample document:\")\n",
    "print(sample['documents'][:500] + \"...\")\n",
    "print(\"\\nSample title:\")\n",
    "print(sample['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWLcE84KTxxv"
   },
   "source": [
    "We'll now process the dataset and retain only the `documents` field because we are\n",
    "fine-tuning the model on the next word prediction task. Take a subset\n",
    "of the dataset for the purpose of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2mwniK-STxxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 619 examples\n",
      "Using 200 examples\n",
      "Training batches: 50\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocess dataset\n",
    "def tokenize_function(examples):\n",
    "    # Use only the documents field\n",
    "    texts = examples['documents']\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Adjust sample size based on available data\n",
    "available_samples = len(dataset)\n",
    "total_needed = NUM_BATCHES * BATCH_SIZE\n",
    "actual_samples = min(available_samples, total_needed)\n",
    "\n",
    "print(f\"Dataset has {available_samples} examples\")\n",
    "print(f\"Using {actual_samples} examples\")\n",
    "\n",
    "# Take subset and tokenize\n",
    "small_dataset = dataset.select(range(actual_samples))\n",
    "tokenized_dataset = small_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=small_dataset.column_names\n",
    ")\n",
    "\n",
    "# Convert to PyTorch dataset\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb4gcgazTxxv"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "Before we begin fine-tuning the models, let's define a few helper functions and\n",
    "classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zw0zDMcTxxv"
   },
   "source": [
    "### Callback for tracking GPU memory usage\n",
    "\n",
    "We'll define a custom callback function which tracks GPU memory usage using\n",
    "PyTorch's memory management functions.\n",
    "\n",
    "Here, we assume that we are using a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_TquTzqgTxxv"
   },
   "outputs": [],
   "source": [
    "class GPUMemoryTracker:\n",
    "    def __init__(self, target_batches, print_stats=False):\n",
    "        self.target_batches = target_batches\n",
    "        self.print_stats = print_stats\n",
    "        self.memory_usage = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def _compute_memory_usage(self):\n",
    "        if torch.cuda.is_available():\n",
    "            # Convert bytes to GB\n",
    "            peak_usage = torch.cuda.max_memory_allocated() / (2**30)\n",
    "            self.memory_usage.append(round(peak_usage, 3))\n",
    "            \n",
    "            if self.print_stats:\n",
    "                current_usage = torch.cuda.memory_allocated() / (2**30)\n",
    "                print(f\"Current memory: {current_usage:.3f}GB, Peak memory: {peak_usage:.3f}GB\")\n",
    "    \n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} start\")\n",
    "    \n",
    "    def on_batch_begin(self, batch):\n",
    "        if batch in self.target_batches:\n",
    "            self._compute_memory_usage()\n",
    "            self.labels.append(f\"batch {batch}\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0OKKtvzTxxv"
   },
   "source": [
    "### Function for text generation\n",
    "\n",
    "Here is a helper function to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QEkGxfP8Txxw"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, input_text, max_length=200, device='cuda'):\n",
    "    start = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nOutput:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Total Time Elapsed: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_NLFa-DTxxw"
   },
   "source": [
    "### Define optimizer and scheduler\n",
    "\n",
    "We will use AdamW optimizer and linear learning rate scheduler for training both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4FMKHT68Txxw"
   },
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model, num_training_steps):\n",
    "    # Separate parameters for weight decay\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=LEARNING_RATE,\n",
    "        eps=1e-6\n",
    "    )\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEwhZehCTxxw"
   },
   "source": [
    "## Fine-tune GPT-2\n",
    "\n",
    "Let's load the model first. We use a sequence length of 128\n",
    "instead of 1024 (which is the default sequence length). This will limit our\n",
    "ability to predict long sequences, but will allow us to run this example quickly\n",
    "on most GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vsjw4DUnTxxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to cuda\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "\n",
    "print(f\"Model loaded to {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in gpt2_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pQjZCOLTxxw"
   },
   "source": [
    "Initialize the GPU memory tracker, optimizer, and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b76wkQziTxxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 500\n"
     ]
    }
   ],
   "source": [
    "# Initialize memory tracker\n",
    "gpu_memory_tracker = GPUMemoryTracker(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "num_training_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Get optimizer and scheduler\n",
    "optimizer, scheduler = get_optimizer_and_scheduler(gpt2_model, num_training_steps)\n",
    "\n",
    "print(f\"Total training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S4HF6rUTxxw"
   },
   "source": [
    "We are all set to train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLeanup GPU (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Starting GPU memory cleanup...\n",
      "   ✓ Deleted gpt2_model\n",
      "   ✓ Deleted optimizer\n",
      "   ✓ Deleted tokenizer\n",
      "📊 Deleted 3 variables\n",
      "🗑️ Running garbage collection...\n",
      "   Cycle 1: Collected 787 objects\n",
      "   Cycle 2: Collected 0 objects\n",
      "   Cycle 3: Collected 0 objects\n",
      "🔥 Clearing CUDA cache...\n",
      "📈 Current GPU memory status:\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved: 0.00 GB\n",
      "   Max allocated: 0.00 GB\n",
      "   Total GPU memory: 7.62 GB\n",
      "   🎯 Free memory: 7.62 GB\n",
      "⚙️ Setting memory optimization flags...\n",
      "✅ GPU memory cleanup completed!\n",
      "\n",
      "🚀 You can now run your training code with a clean GPU state.\n",
      "\n",
      "💡 Recommended next steps:\n",
      "   1. Restart your kernel for the cleanest start (optional but recommended)\n",
      "   2. Use the memory-optimized hyperparameters:\n",
      "      - BATCH_SIZE = 2 or 4\n",
      "      - MAX_SEQUENCE_LENGTH = 64\n",
      "      - Use gradient accumulation\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE GPU MEMORY CLEARANCE CODE BLOCK\n",
    "# Run this in a new cell to completely clear GPU memory\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"🧹 Starting GPU memory cleanup...\")\n",
    "\n",
    "# Step 1: Delete all model-related variables\n",
    "variables_to_delete = [\n",
    "    'gpt2_model', 'trained_gpt2', 'base_model', 'lora_model', 'loaded_lora_model',\n",
    "    'optimizer', 'scheduler', 'lora_optimizer', 'lora_scheduler',\n",
    "    'train_dataloader', 'dataset', 'tokenized_dataset', 'small_dataset',\n",
    "    'tokenizer', 'scaler', 'gpu_memory_tracker', 'lora_memory_tracker',\n",
    "    'outputs', 'loss', 'input_ids', 'attention_mask', 'labels'\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "for var_name in variables_to_delete:\n",
    "    if var_name in locals():\n",
    "        exec(f\"del {var_name}\")\n",
    "        deleted_count += 1\n",
    "        print(f\"   ✓ Deleted {var_name}\")\n",
    "    elif var_name in globals():\n",
    "        exec(f\"del {var_name}\")\n",
    "        deleted_count += 1\n",
    "        print(f\"   ✓ Deleted {var_name} (global)\")\n",
    "\n",
    "print(f\"📊 Deleted {deleted_count} variables\")\n",
    "\n",
    "# Step 2: Force garbage collection\n",
    "print(\"🗑️ Running garbage collection...\")\n",
    "for i in range(3):  # Run multiple times for thorough cleanup\n",
    "    collected = gc.collect()\n",
    "    print(f\"   Cycle {i+1}: Collected {collected} objects\")\n",
    "\n",
    "# Step 3: Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    print(\"🔥 Clearing CUDA cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # Reset CUDA memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    \n",
    "    print(\"📈 Current GPU memory status:\")\n",
    "    allocated = torch.cuda.memory_allocated() / (1024**3)  # GB\n",
    "    reserved = torch.cuda.memory_reserved() / (1024**3)   # GB\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "    \n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\") \n",
    "    print(f\"   Max allocated: {max_allocated:.2f} GB\")\n",
    "    print(f\"   Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Calculate free memory\n",
    "    free_memory = (torch.cuda.get_device_properties(0).total_memory / (1024**3)) - reserved\n",
    "    print(f\"   🎯 Free memory: {free_memory:.2f} GB\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ CUDA not available\")\n",
    "\n",
    "# Step 4: Set memory optimization environment variables\n",
    "print(\"⚙️ Setting memory optimization flags...\")\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For debugging if needed\n",
    "\n",
    "print(\"✅ GPU memory cleanup completed!\")\n",
    "print(\"\\n🚀 You can now run your training code with a clean GPU state.\")\n",
    "print(\"\\n💡 Recommended next steps:\")\n",
    "print(\"   1. Restart your kernel for the cleanest start (optional but recommended)\")\n",
    "print(\"   2. Use the memory-optimized hyperparameters:\")\n",
    "print(\"      - BATCH_SIZE = 2 or 4\")\n",
    "print(\"      - MAX_SEQUENCE_LENGTH = 64\")\n",
    "print(\"      - Use gradient accumulation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "45qyLnw8Txxx"
   },
   "outputs": [],
   "source": [
    "def train_model_memory_optimized(model, dataloader, optimizer, scheduler, memory_tracker, epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        memory_tracker.on_epoch_begin(epoch)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Initialize gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            memory_tracker.on_batch_begin(batch_idx)\n",
    "            \n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS  # Scale loss\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Only step optimizer every GRADIENT_ACCUMULATION_STEPS\n",
    "            if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(dataloader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "            progress_bar.set_postfix({'loss': loss.item() * GRADIENT_ACCUMULATION_STEPS})\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 5 == 0:  # More frequent clearing\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        memory_tracker.on_epoch_end(epoch)\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    return model, memory_tracker.memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training...\n",
      "Current memory: 0.476GB, Peak memory: 0.952GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fc97312ebe4b76a51c2014a7137f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 0.985GB, Peak memory: 1.354GB\n",
      "Current memory: 0.985GB, Peak memory: 1.354GB\n",
      "Current memory: 0.986GB, Peak memory: 1.358GB\n",
      "Current memory: 1.447GB, Peak memory: 2.380GB\n",
      "Epoch 1 average loss: 7.9177\n",
      "Current memory: 1.447GB, Peak memory: 2.380GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04790ff48d1a40a4bbe799489ba025c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.380GB\n",
      "Current memory: 1.915GB, Peak memory: 2.380GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 2 average loss: 6.8048\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99efe4932e3d47858badbe05a44b6902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 3 average loss: 3.9832\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d56fc6d8394560a27348c626abc7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 4 average loss: 1.8647\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe66317f035b486dad6bcb2efa631859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 5 average loss: 1.1208\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44105ed76d024f77b75ec0140ac75f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 6 average loss: 0.8640\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d143a9c41a24211b35b26e06a38495e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 7 average loss: 0.7664\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a97f1239448bea7d0a0bedaac4b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 8 average loss: 0.7079\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0a05e61bfe47ea8420444564c3f636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 9 average loss: 0.6696\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9b8b6f16e1442081be6ddb2b52b589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.915GB, Peak memory: 2.383GB\n",
      "Current memory: 1.916GB, Peak memory: 2.383GB\n",
      "Current memory: 1.447GB, Peak memory: 2.383GB\n",
      "Epoch 10 average loss: 0.6189\n",
      "✅ Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to ./my-fine-tuned-gpt2\n",
      "\n",
      "🎯 Testing:\n",
      "\n",
      "Output:\n",
      "I like basketball.\n",
      "Total Time Elapsed: 0.15s\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Load GPT-2 model & tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Define optimizer & scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    [p for p in gpt2_model.parameters() if p.requires_grad],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "num_training_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "\n",
    "# 3. Train and then save\n",
    "print(\"🚀 Starting training...\")\n",
    "trained_gpt2, _ = train_model_memory_optimized(\n",
    "    gpt2_model,\n",
    "    train_dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    gpu_memory_tracker,\n",
    "    EPOCHS\n",
    ")\n",
    "print(\"✅ Training completed!\")\n",
    "\n",
    "def save_full_gpt2_model(model, tokenizer, save_path=\"./my-fine-tuned-gpt2\"):\n",
    "    import os\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"✅ Saved to {save_path}\")\n",
    "\n",
    "save_full_gpt2_model(trained_gpt2, tokenizer)\n",
    "\n",
    "print(\"\\n🎯 Testing:\")\n",
    "generate_text(trained_gpt2, tokenizer, \"I like basketball\", max_length=MAX_GENERATION_LENGTH, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN1eJIF_Txxx"
   },
   "source": [
    "As a final step, let's generate some text. The\n",
    "first call to `generate()` might be slow due to CUDA kernel initialization, but\n",
    "subsequent calls will be faster. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pGza4-CKTxxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with base GPT-2...\n",
      "\n",
      "Output:\n",
      "I like basketball so much I just bought a new pair of sneakers\n",
      "Total Time Elapsed: 0.05s\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating text with base GPT-2...\")\n",
    "generate_text(gpt2_model, tokenizer, \"I like basketball\", max_length=MAX_GENERATION_LENGTH, device=device)\n",
    "#generate_text(gpt2_model, tokenizer, \"That Italian restaurant is\", max_length=MAX_GENERATION_LENGTH, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with loaded fine-tuned GPT-2…\n",
      "\n",
      "Output:\n",
      "I like Chocolate, but I like that I can get the right color in my eye\n",
      "Total Time Elapsed: 0.06s\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Load from disk\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./my-fine-tuned-gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./my-fine-tuned-gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # if needed\n",
    "# Then generate\n",
    "print(\"Generating text with loaded fine-tuned GPT-2…\")\n",
    "generate_text(model, tokenizer, \"I like Chocolate\", max_length=MAX_GENERATION_LENGTH, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yee_V2ITxxx"
   },
   "source": [
    "## LoRA GPT-2\n",
    "\n",
    "In this section, we discuss the technical details of LoRA, build a LoRA GPT-2\n",
    "model using PEFT library, fine-tune it and generate text.\n",
    "\n",
    "### What exactly is LoRA?\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning technique for LLMs. It freezes the\n",
    "weights of the LLM, and injects trainable rank-decomposition matrices. Let's\n",
    "understand this more clearly.\n",
    "\n",
    "Assume we have an `n x n` pre-trained dense layer (or weight matrix), `W0`. We\n",
    "initialize two dense layers, `A` and `B`, of shapes `n x rank`, and `rank x n`,\n",
    "respectively. `rank` is much smaller than `n`. In the paper, values between 1\n",
    "and 4 are shown to work well.\n",
    "\n",
    "#### LoRA equation\n",
    "\n",
    "The original equation is `output = W0x + b0`, where `x` is the input, `W0` and\n",
    "`b0` are the weight matrix and bias terms of the original dense layer (frozen).\n",
    "The LoRA equation is: `output = W0x + b0 + BAx`, where `A` and `B` are the\n",
    "rank-decomposition matrices.\n",
    "\n",
    "LoRA is based on the idea that updates to the weights of the pre-trained\n",
    "language model have a low \"intrinsic rank\" since pre-trained language models are\n",
    "over-parametrized. Predictive performance of full fine-tuning can be replicated\n",
    "even by constraining `W0`'s updates to low-rank decomposition matrices.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.imgur.com/f4TFqMi.png\" alt=\"lora_diagram\" height=\"250\"/>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "#### Number of trainable parameters\n",
    "\n",
    "Let's do some quick math. Suppose `n` is 768, and `rank` is 4. `W0` has\n",
    "`768 x 768 = 589,824` parameters, whereas the LoRA layers, `A` and `B` together\n",
    "have `768 x 4 + 4 x 768 = 6,144` parameters. So, for the dense layer, we go from\n",
    "`589,824` trainable parameters to `6,144` trainable parameters!\n",
    "\n",
    "#### Why does LoRA reduce memory footprint?\n",
    "\n",
    "Even though the total number of parameters increase (since we are adding LoRA\n",
    "layers), the memory footprint reduces, because the number of trainable\n",
    "parameters reduces. Let's dive deeper into this.\n",
    "\n",
    "The memory usage of a model can be split into four parts:\n",
    "\n",
    "- Model memory: This is the memory required to store the model weights. This\n",
    "will be slightly higher for LoRA than GPT-2.\n",
    "- Forward pass memory: This mostly depends on batch size, sequence length, etc.\n",
    "We keep this constant for both models for a fair comparison.\n",
    "- Backward pass memory: This is the memory required to store the gradients.\n",
    "Note that the gradients are computed only for the trainable parameters.\n",
    "- Optimizer memory: This is the memory required to store the optimizer state.\n",
    "For example, the Adam optimizer stores the \"1st moment vectors\" and\n",
    "\"2nd moment vectors\" for the trainable parameters.\n",
    "\n",
    "Since, with LoRA, there is a huge reduction in the number of trainable\n",
    "parameters, the optimizer memory and the memory required to store the gradients\n",
    "for LoRA is much less than GPT-2. This is where most of the memory savings\n",
    "happen.\n",
    "\n",
    "#### Why is LoRA so popular?\n",
    "\n",
    "- Reduces GPU memory usage;\n",
    "- Faster training; and\n",
    "- No additional inference latency.\n",
    "\n",
    "### Create LoRA Model using PEFT\n",
    "\n",
    "We'll use Hugging Face's PEFT library to create a LoRA version of GPT-2.\n",
    "The PEFT library handles all the complexity of injecting LoRA adapters into\n",
    "the transformer layers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zjkyd6HWTxxt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared. Loading fresh GPT-2 for LoRA...\n"
     ]
    }
   ],
   "source": [
    "# Clean up memory from previous model\n",
    "del trained_gpt2\n",
    "del optimizer\n",
    "del scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared. Loading fresh GPT-2 for LoRA...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30LpqmvQTxxw"
   },
   "source": [
    "### Configure and create LoRA model\n",
    "\n",
    "We'll configure LoRA to target the attention layers (query and value projections)\n",
    "of GPT-2, which is typically where LoRA shows the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zjkyd6HWTxxt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405,504 || all params: 124,845,312 || trainable%: 0.3248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanix/Documents/JupyterEnv/zaspervenv1/lib/python3.13/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a fresh GPT-2 model for LoRA\n",
    "base_model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=LORA_RANK,  # rank\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # target attention modules in GPT-2\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model = lora_model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34pXckGpTxxw"
   },
   "source": [
    "Let's do a forward pass to make sure we have a valid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rVGCkn1GTxxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful! Output shape: torch.Size([1, 12, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "test_input = tokenizer(\"LoRA is very useful for quick LLM finetuning\", return_tensors='pt')\n",
    "test_input = {k: v.to(device) for k, v in test_input.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model(**test_input)\n",
    "\n",
    "print(f\"Forward pass successful! Output shape: {outputs.logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40CZsC1mTxxw"
   },
   "source": [
    "### Fine-tune LoRA GPT-2\n",
    "\n",
    "Now that we have created the LoRA GPT-2 model, let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LoRA model...\n",
      "✅ Input gradients enabled\n",
      "trainable params: 589,824 || all params: 125,029,632 || trainable%: 0.4717\n",
      "LoRA Model created successfully!\n",
      "Total parameters: 125,029,632\n",
      "Trainable parameters: 589,824\n",
      "Starting LoRA GPT-2 training...\n",
      "Current memory: 1.932GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780434691d8141db8cfe202cf852b062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.960GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 1 average loss: 7.8845\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e47910e912a4ce18a321b14e29a4803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 2 average loss: 7.8842\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf6cd103e314b3ea5ea9cf5f2c9f3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 3 average loss: 7.7845\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb1144d52804c0b8e640ca92392f2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 4 average loss: 7.6500\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0431984e9f401ab3ef603db78e7dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 5 average loss: 7.4643\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164d300a5eba4eef8e02f58629b0f6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 6 average loss: 7.1209\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d87fda378d46108297067beb76cae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 7 average loss: 6.5786\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0fbd952add4cf481d69922c2faaae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 8 average loss: 5.8395\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bb3267861046c0a87908761abd4a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 9 average loss: 4.7606\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8452eccc8dd4c4aa69dc1d8a0880446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.964GB, Peak memory: 2.383GB\n",
      "Current memory: 1.962GB, Peak memory: 2.383GB\n",
      "Epoch 10 average loss: 3.5680\n",
      "LoRA GPT-2 training completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize memory tracker for LoRA\n",
    "lora_memory_tracker = GPUMemoryTracker(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "print(\"Creating LoRA model...\")\n",
    "\n",
    "# STEP 1: Load base model\n",
    "base_model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# STEP 2: CRITICAL FIX - Enable input gradients BEFORE applying LoRA\n",
    "if hasattr(base_model, \"enable_input_require_grads\"):\n",
    "    base_model.enable_input_require_grads()\n",
    "else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "        output.requires_grad_(True)\n",
    "    base_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "print(\"✅ Input gradients enabled\")\n",
    "\n",
    "# STEP 3: Create LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# STEP 4: Apply LoRA to the model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model.train()\n",
    "\n",
    "# STEP 5: Verify trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "print(f\"LoRA Model created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in lora_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Get optimizer and scheduler for LoRA model\n",
    "lora_optimizer, lora_scheduler = get_optimizer_and_scheduler(lora_model, num_training_steps)\n",
    "\n",
    "print(\"Starting LoRA GPT-2 training...\")\n",
    "trained_lora_model, lora_memory_usage = train_model_memory_optimized(\n",
    "    lora_model, train_dataloader, lora_optimizer, lora_scheduler, lora_memory_tracker, EPOCHS\n",
    ")\n",
    "print(\"LoRA GPT-2 training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42CZsC1mTxxw"
   },
   "source": [
    "### Compare memory usage and performance\n",
    "\n",
    "Let's compare the memory usage between standard fine-tuning and LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt2_memory_usage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Plot memory usage comparison\u001b[39;00m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      3\u001b[39m plt.bar(\n\u001b[32m      4\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mStandard GPT-2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLoRA GPT-2\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     [\u001b[38;5;28mmax\u001b[39m(\u001b[43mgpt2_memory_usage\u001b[49m), \u001b[38;5;28mmax\u001b[39m(lora_memory_usage)],\n\u001b[32m      6\u001b[39m     color=[\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     alpha=\u001b[32m0.7\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mModel Type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mPeak GPU Memory Usage (GB)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'gpt2_memory_usage' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot memory usage comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    [\"Standard GPT-2\", \"LoRA GPT-2\"],\n",
    "    [max(gpt2_memory_usage), max(lora_memory_usage)],\n",
    "    color=[\"red\", \"blue\"],\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.ylabel(\"Peak GPU Memory Usage (GB)\")\n",
    "plt.title(\"GPU Memory Usage Comparison\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate([max(gpt2_memory_usage), max(lora_memory_usage)]):\n",
    "    plt.text(i, v + 0.1, f'{v:.2f}GB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate memory savings\n",
    "memory_savings = ((max(gpt2_memory_usage) - max(lora_memory_usage)) / max(gpt2_memory_usage)) * 100\n",
    "print(f\"\\nMemory savings with LoRA: {memory_savings:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46CZsC1mTxxw"
   },
   "source": [
    "### Generate text with LoRA model\n",
    "\n",
    "Let's generate text with our fine-tuned LoRA model. One of the advantages of LoRA\n",
    "is that there's no additional inference latency compared to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with LoRA fine-tuned GPT-2...\n",
      "\n",
      "Output:\n",
      "I like basketball. I like baseball. I like tennis. I like to play with my friends. I don't know what it is, but I like basketball. I like baseball. I like tennis. I like to play with my friends. I don't know what it is, but I like basketball.\n",
      "Total Time Elapsed: 0.37s\n",
      "\n",
      "Output:\n",
      "That Italian restaurant is the best in town, and the best in Italy.\n",
      "\n",
      "I'm a little confused, but I think the Italian restaurant is the best in town, and the best in Italy.\n",
      "\n",
      "I like the fact that the interior is not as cramped as it should be, and the food is good. I've heard that the pizza, pasta and cheese are pretty good, but that's because the Italian restaurant is a little bit cramped, and the restaurant has a lot of extra stuff, like a lot of people are coming.\n",
      "\n",
      "I like the fact that the interior is not as cramped as it should be, and the food is good. I've heard that the pizza, pasta and cheese are pretty good, but that's because the Italian restaurant is a little bit cramped, and the restaurant has a lot of extra stuff, like a lot of people are coming.\n",
      "\n",
      "I don't have the money to buy anything on the menu, but I'm happy to get\n",
      "Total Time Elapsed: 1.19s\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating text with LoRA fine-tuned GPT-2...\")\n",
    "generate_text(trained_lora_model, tokenizer, \"I like basketball\", max_length=MAX_GENERATION_LENGTH, device=device)\n",
    "generate_text(trained_lora_model, tokenizer, \"That Italian restaurant is\", max_length=MAX_GENERATION_LENGTH, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48CZsC1mTxxw"
   },
   "source": [
    "### Save and Load LoRA Adapters\n",
    "\n",
    "One of the benefits of LoRA is that you can save only the adapter weights (which are much smaller)\n",
    "and load them on top of the base model when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters saved to ./gpt2-lora-reddit\n",
      "LoRA adapter size: 2.27 MB\n",
      "Compare this to the full GPT-2 model which is ~500MB!\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "trained_lora_model.save_pretrained(\"./gpt2-lora-reddit\")\n",
    "print(\"LoRA adapters saved to ./gpt2-lora-reddit\")\n",
    "\n",
    "# Calculate adapter size\n",
    "import os\n",
    "adapter_size = sum(\n",
    "    os.path.getsize(os.path.join(\"./gpt2-lora-reddit\", f)) \n",
    "    for f in os.listdir(\"./gpt2-lora-reddit\") \n",
    "    if os.path.isfile(os.path.join(\"./gpt2-lora-reddit\", f))\n",
    ") / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "print(f\"LoRA adapter size: {adapter_size:.2f} MB\")\n",
    "print(\"Compare this to the full GPT-2 model which is ~500MB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50CZsC1mTxxw"
   },
   "source": [
    "### Load LoRA adapters (demonstration)\n",
    "\n",
    "Here's how you would load the LoRA adapters in a new session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA model loaded successfully!\n",
      "\n",
      "Output:\n",
      "Today I learned that when I am in a romantic relationship, you can expect to get a lot of love from my love life. But I don't feel like that's what most people do.\n",
      "\n",
      "I think it's important to be honest with yourself. I don't want to be the one saying 'I don't want to have any sex with you because of your body,' but I'm sure you can see that I am the opposite. You know, I don't have sex with you because you are a little bit too big. I don't have sex with you because I am so small. I don't have sex with you because I am so small.\n",
      "\n",
      "I think it's important to be honest with yourself. I don\n",
      "Total Time Elapsed: 0.87s\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model_for_inference = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "\n",
    "# Load LoRA adapters\n",
    "loaded_lora_model = PeftModel.from_pretrained(base_model_for_inference, \"./gpt2-lora-reddit\")\n",
    "loaded_lora_model = loaded_lora_model.to(device)\n",
    "\n",
    "print(\"LoRA model loaded successfully!\")\n",
    "\n",
    "# Test generation\n",
    "generate_text(loaded_lora_model, tokenizer, \"Today I learned that\", max_length=150, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52CZsC1mTxxw"
   },
   "source": [
    "## Summary and Comparison\n",
    "\n",
    "In this notebook, we've successfully demonstrated:\n",
    "\n",
    "1. **Standard Fine-tuning**: Traditional approach where all model parameters are updated\n",
    "2. **LoRA Fine-tuning**: Parameter-efficient approach using low-rank adaptation\n",
    "\n",
    "### Key Benefits of LoRA:\n",
    "\n",
    "- **Memory Efficiency**: Significantly reduced GPU memory usage during training\n",
    "- **Storage Efficiency**: LoRA adapters are much smaller than full model checkpoints\n",
    "- **Training Speed**: Faster training due to fewer parameters to update\n",
    "- **No Inference Overhead**: Same inference speed as the original model\n",
    "- **Modularity**: Easy to switch between different LoRA adapters for different tasks\n",
    "\n",
    "### When to use LoRA:\n",
    "\n",
    "- Limited GPU memory\n",
    "- Multiple task-specific adaptations needed\n",
    "- Quick experimentation and prototyping\n",
    "- Fine-tuning large models on consumer hardware\n",
    "\n",
    "This PyTorch implementation provides a complete, production-ready approach to parameter-efficient fine-tuning with LoRA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54CZsC1mTxxw"
   },
   "source": [
    "## Additional Utilities\n",
    "\n",
    "Here are some additional utility functions that might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Statistics:\n",
      "LoRA Model:\n",
      "Total parameters: 125,029,632\n",
      "Trainable parameters: 0\n",
      "Trainable %: 0.00%\n",
      "Model size: 488.95 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Load base GPT-2\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# 2. Load your LoRA adapter from the directory containing adapter_config.json and adapter_model.safetensors\n",
    "loaded_lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./gpt2-lora-reddit\",\n",
    "    from_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "# 3. Print stats\n",
    "print(\"Model Statistics:\\nLoRA Model:\")\n",
    "count_parameters(loaded_lora_model)\n",
    "print(f\"Model size: {get_model_size_mb(loaded_lora_model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56CZsC1mTxxw"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has successfully demonstrated how to implement parameter-efficient fine-tuning of GPT-2 using LoRA in PyTorch. The implementation is optimized for Linux systems with CUDA support and provides:\n",
    "\n",
    "- Complete PyTorch port from the original TensorFlow/Keras implementation\n",
    "- Modern best practices using Hugging Face Transformers and PEFT\n",
    "- Memory-efficient training with mixed precision\n",
    "- Comprehensive comparison between standard and LoRA fine-tuning\n",
    "- Practical utilities for model analysis and deployment\n",
    "\n",
    "LoRA continues to be one of the most effective parameter-efficient fine-tuning techniques, enabling efficient adaptation of large language models even on resource-constrained hardware.\n",
    "\n",
    "Happy fine-tuning! 🚀"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "parameter_efficient_finetuning_of_gpt2_with_lora_pytorch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "zaspervenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
