{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DrjAmiETxxr"
   },
   "source": [
    "# Parameter-efficient fine-tuning of GPT-2 with LoRA (PyTorch)\n",
    "\n",
    "**Description:** Use PyTorch and PEFT to fine-tune a GPT-2 LLM with LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUDah9mYTxxs"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) have been shown to be effective at a variety of NLP\n",
    "tasks. An LLM is first pre-trained on a large corpus of text in a\n",
    "self-supervised fashion. Pre-training helps LLMs learn general-purpose knowledge,\n",
    "such as statistical relationships between words. An LLM can then be fine-tuned\n",
    "on a downstream task of interest (such as sentiment analysis).\n",
    "\n",
    "However, LLMs are extremely large in size, and we don't need to train all the\n",
    "parameters in the model while fine-tuning, especially because datasets on which\n",
    "the model is fine-tuned are relatively small. Another way of saying this is\n",
    "that LLMs are over-parametrized for fine-tuning. This is where\n",
    "[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685) comes in; it\n",
    "significantly reduces the number of trainable parameters. This results in a\n",
    "decrease in training time and GPU memory usage, while maintaining the quality\n",
    "of the outputs.\n",
    "\n",
    "In this example, we will explain LoRA in technical terms, show how the technical\n",
    "explanation translates to code, use PyTorch and Hugging Face's PEFT library to implement\n",
    "[GPT-2 model](https://huggingface.co/gpt2) and fine-tune\n",
    "it on the next token prediction task using LoRA. We will compare LoRA GPT-2\n",
    "with a fully fine-tuned GPT-2 in terms of the quality of the generated text,\n",
    "training time and GPU memory usage.\n",
    "\n",
    "Note: This example runs on PyTorch with CUDA support optimized for Linux systems\n",
    "with CUDA 12.x compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RbBiRktTxxs"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before we start implementing the pipeline, let's install and import all the\n",
    "libraries we need. We'll be using PyTorch, Transformers, and PEFT libraries.\n",
    "\n",
    "Secondly, let's enable mixed precision training. This will help us reduce the\n",
    "training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6QJI-XewTxxs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: peft in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (4.55.4)\n",
      "Requirement already satisfied: tqdm in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2025.7.0)\n",
      "Requirement already satisfied: requests in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.8)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers->peft) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from transformers->peft) (0.21.4)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Using cached yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Using cached propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:26\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspecâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/14\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/14\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/14\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/14\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/14\u001b[0m [datasets]/14\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Requirement already satisfied: accelerate in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.8)\n",
      "Requirement already satisfied: setuptools in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Requirement already satisfied: matplotlib in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages optimized for CUDA 12.x\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zjkyd6HWTxxt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heytanix/Documents/Jain_PCL/PCL_repository/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA Version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Enable mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTD_dPMtTxxt"
   },
   "source": [
    "Let's also define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dOs9MTZnTxxt"
   },
   "outputs": [],
   "source": [
    "# General hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_BATCHES = 500\n",
    "EPOCHS = 1  # Can be set to a higher value for better results\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "MAX_GENERATION_LENGTH = 200\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "GPT2_MODEL_NAME = \"gpt2\"\n",
    "\n",
    "# LoRA-specific hyperparameters\n",
    "LORA_RANK = 4\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W8007pgTxxu"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load a Reddit dataset. We will fine-tune both the GPT-2 model and the\n",
    "LoRA GPT-2 model on a subset of this dataset. The aim is to produce text similar\n",
    "in style to Reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F-c7yXqdTxxu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 619/619 [00:00<00:00, 131944.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 619/619 [00:00<00:00, 46717.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Reddit TIFU dataset\n",
    "try:\n",
    "    dataset = load_dataset(\"Fredithefish/Reddit-TIFU\", split=\"train\")\n",
    "    \n",
    "    # Map to expected 'documents' field if needed\n",
    "    if 'documents' not in dataset.column_names:\n",
    "        def map_to_documents(example):\n",
    "            return {'documents': example.get('selftext', example.get('title', ''))}\n",
    "        dataset = dataset.map(map_to_documents)\n",
    "        \n",
    "except:\n",
    "    # Fallback to custom dataset if download fails\n",
    "    from datasets import Dataset\n",
    "    texts = [\"TIFU by accidentally sending an embarrassing text to the wrong person.\"] * (NUM_BATCHES * BATCH_SIZE)\n",
    "    dataset = Dataset.from_dict({'documents': texts})\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtOROLo4Txxu"
   },
   "source": [
    "The dataset has two fields: `documents` and `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tm45e-LITxxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "TIFU by raising the flag upside down on a military base and causing local farmers to think the base was in distress....\n",
      "\n",
      "Sample title:\n",
      "TIFU by raising the flag upside down on a military base and causing local farmers to think the base was in distress.\n"
     ]
    }
   ],
   "source": [
    "# Examine dataset structure\n",
    "sample = dataset[0]\n",
    "print(\"Sample document:\")\n",
    "print(sample['documents'][:500] + \"...\")\n",
    "print(\"\\nSample title:\")\n",
    "print(sample['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWLcE84KTxxv"
   },
   "source": [
    "We'll now process the dataset and retain only the `documents` field because we are\n",
    "fine-tuning the model on the next word prediction task. Take a subset\n",
    "of the dataset for the purpose of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2mwniK-STxxv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 619 examples\n",
      "Requested 16000 examples\n",
      "Using 619 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 619/619 [00:00<00:00, 9436.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocess dataset\n",
    "def tokenize_function(examples):\n",
    "    # Use only the documents field\n",
    "    texts = examples['documents']\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Adjust sample size based on available data\n",
    "available_samples = len(dataset)\n",
    "total_needed = NUM_BATCHES * BATCH_SIZE\n",
    "actual_samples = min(available_samples, total_needed)\n",
    "\n",
    "print(f\"Dataset has {available_samples} examples\")\n",
    "print(f\"Requested {total_needed} examples\")\n",
    "print(f\"Using {actual_samples} examples\")\n",
    "\n",
    "# Take subset and tokenize\n",
    "small_dataset = dataset.select(range(actual_samples))\n",
    "tokenized_dataset = small_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=small_dataset.column_names\n",
    ")\n",
    "\n",
    "# Convert to PyTorch dataset\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb4gcgazTxxv"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "Before we begin fine-tuning the models, let's define a few helper functions and\n",
    "classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zw0zDMcTxxv"
   },
   "source": [
    "### Callback for tracking GPU memory usage\n",
    "\n",
    "We'll define a custom callback function which tracks GPU memory usage using\n",
    "PyTorch's memory management functions.\n",
    "\n",
    "Here, we assume that we are using a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_TquTzqgTxxv"
   },
   "outputs": [],
   "source": [
    "class GPUMemoryTracker:\n",
    "    def __init__(self, target_batches, print_stats=False):\n",
    "        self.target_batches = target_batches\n",
    "        self.print_stats = print_stats\n",
    "        self.memory_usage = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def _compute_memory_usage(self):\n",
    "        if torch.cuda.is_available():\n",
    "            # Convert bytes to GB\n",
    "            peak_usage = torch.cuda.max_memory_allocated() / (2**30)\n",
    "            self.memory_usage.append(round(peak_usage, 3))\n",
    "            \n",
    "            if self.print_stats:\n",
    "                current_usage = torch.cuda.memory_allocated() / (2**30)\n",
    "                print(f\"Current memory: {current_usage:.3f}GB, Peak memory: {peak_usage:.3f}GB\")\n",
    "    \n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} start\")\n",
    "    \n",
    "    def on_batch_begin(self, batch):\n",
    "        if batch in self.target_batches:\n",
    "            self._compute_memory_usage()\n",
    "            self.labels.append(f\"batch {batch}\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch):\n",
    "        self._compute_memory_usage()\n",
    "        self.labels.append(f\"epoch {epoch} end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0OKKtvzTxxv"
   },
   "source": [
    "### Function for text generation\n",
    "\n",
    "Here is a helper function to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QEkGxfP8Txxw"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, input_text, max_length=200, device='cuda'):\n",
    "    start = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nOutput:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Total Time Elapsed: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_NLFa-DTxxw"
   },
   "source": [
    "### Define optimizer and scheduler\n",
    "\n",
    "We will use AdamW optimizer and linear learning rate scheduler for training both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4FMKHT68Txxw"
   },
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model, num_training_steps):\n",
    "    # Separate parameters for weight decay\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=LEARNING_RATE,\n",
    "        eps=1e-6\n",
    "    )\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEwhZehCTxxw"
   },
   "source": [
    "## Fine-tune GPT-2\n",
    "\n",
    "Let's load the model first. We use a sequence length of 128\n",
    "instead of 1024 (which is the default sequence length). This will limit our\n",
    "ability to predict long sequences, but will allow us to run this example quickly\n",
    "on most GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vsjw4DUnTxxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to cuda\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "\n",
    "print(f\"Model loaded to {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in gpt2_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pQjZCOLTxxw"
   },
   "source": [
    "Initialize the GPU memory tracker, optimizer, and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b76wkQziTxxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 20\n"
     ]
    }
   ],
   "source": [
    "# Initialize memory tracker\n",
    "gpu_memory_tracker = GPUMemoryTracker(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "num_training_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Get optimizer and scheduler\n",
    "optimizer, scheduler = get_optimizer_and_scheduler(gpt2_model, num_training_steps)\n",
    "\n",
    "print(f\"Total training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S4HF6rUTxxw"
   },
   "source": [
    "We are all set to train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLeanup GPU (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Starting GPU memory cleanup...\n",
      "   âœ“ Deleted gpt2_model\n",
      "   âœ“ Deleted optimizer\n",
      "   âœ“ Deleted scheduler\n",
      "   âœ“ Deleted train_dataloader\n",
      "   âœ“ Deleted dataset\n",
      "   âœ“ Deleted tokenized_dataset\n",
      "   âœ“ Deleted small_dataset\n",
      "   âœ“ Deleted tokenizer\n",
      "   âœ“ Deleted scaler\n",
      "   âœ“ Deleted gpu_memory_tracker\n",
      "ğŸ“Š Deleted 10 variables\n",
      "ğŸ—‘ï¸ Running garbage collection...\n",
      "   Cycle 1: Collected 4637 objects\n",
      "   Cycle 2: Collected 0 objects\n",
      "   Cycle 3: Collected 0 objects\n",
      "ğŸ”¥ Clearing CUDA cache...\n",
      "ğŸ“ˆ Current GPU memory status:\n",
      "   Allocated: 0.66 GB\n",
      "   Reserved: 2.06 GB\n",
      "   Max allocated: 0.66 GB\n",
      "   Total GPU memory: 7.62 GB\n",
      "   ğŸ¯ Free memory: 5.56 GB\n",
      "âš™ï¸ Setting memory optimization flags...\n",
      "âœ… GPU memory cleanup completed!\n",
      "\n",
      "ğŸš€ You can now run your training code with a clean GPU state.\n",
      "\n",
      "ğŸ’¡ Recommended next steps:\n",
      "   1. Restart your kernel for the cleanest start (optional but recommended)\n",
      "   2. Use the memory-optimized hyperparameters:\n",
      "      - BATCH_SIZE = 2 or 4\n",
      "      - MAX_SEQUENCE_LENGTH = 64\n",
      "      - Use gradient accumulation\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE GPU MEMORY CLEARANCE CODE BLOCK\n",
    "# Run this in a new cell to completely clear GPU memory\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"ğŸ§¹ Starting GPU memory cleanup...\")\n",
    "\n",
    "# Step 1: Delete all model-related variables\n",
    "variables_to_delete = [\n",
    "    'gpt2_model', 'trained_gpt2', 'base_model', 'lora_model', 'loaded_lora_model',\n",
    "    'optimizer', 'scheduler', 'lora_optimizer', 'lora_scheduler',\n",
    "    'train_dataloader', 'dataset', 'tokenized_dataset', 'small_dataset',\n",
    "    'tokenizer', 'scaler', 'gpu_memory_tracker', 'lora_memory_tracker',\n",
    "    'outputs', 'loss', 'input_ids', 'attention_mask', 'labels'\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "for var_name in variables_to_delete:\n",
    "    if var_name in locals():\n",
    "        exec(f\"del {var_name}\")\n",
    "        deleted_count += 1\n",
    "        print(f\"   âœ“ Deleted {var_name}\")\n",
    "    elif var_name in globals():\n",
    "        exec(f\"del {var_name}\")\n",
    "        deleted_count += 1\n",
    "        print(f\"   âœ“ Deleted {var_name} (global)\")\n",
    "\n",
    "print(f\"ğŸ“Š Deleted {deleted_count} variables\")\n",
    "\n",
    "# Step 2: Force garbage collection\n",
    "print(\"ğŸ—‘ï¸ Running garbage collection...\")\n",
    "for i in range(3):  # Run multiple times for thorough cleanup\n",
    "    collected = gc.collect()\n",
    "    print(f\"   Cycle {i+1}: Collected {collected} objects\")\n",
    "\n",
    "# Step 3: Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ğŸ”¥ Clearing CUDA cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # Reset CUDA memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "    \n",
    "    print(\"ğŸ“ˆ Current GPU memory status:\")\n",
    "    allocated = torch.cuda.memory_allocated() / (1024**3)  # GB\n",
    "    reserved = torch.cuda.memory_reserved() / (1024**3)   # GB\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "    \n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\") \n",
    "    print(f\"   Max allocated: {max_allocated:.2f} GB\")\n",
    "    print(f\"   Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Calculate free memory\n",
    "    free_memory = (torch.cuda.get_device_properties(0).total_memory / (1024**3)) - reserved\n",
    "    print(f\"   ğŸ¯ Free memory: {free_memory:.2f} GB\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ CUDA not available\")\n",
    "\n",
    "# Step 4: Set memory optimization environment variables\n",
    "print(\"âš™ï¸ Setting memory optimization flags...\")\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For debugging if needed\n",
    "\n",
    "print(\"âœ… GPU memory cleanup completed!\")\n",
    "print(\"\\nğŸš€ You can now run your training code with a clean GPU state.\")\n",
    "print(\"\\nğŸ’¡ Recommended next steps:\")\n",
    "print(\"   1. Restart your kernel for the cleanest start (optional but recommended)\")\n",
    "print(\"   2. Use the memory-optimized hyperparameters:\")\n",
    "print(\"      - BATCH_SIZE = 2 or 4\")\n",
    "print(\"      - MAX_SEQUENCE_LENGTH = 64\")\n",
    "print(\"      - Use gradient accumulation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "45qyLnw8Txxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-optimized GPT-2 training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gpt2_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Use the optimized training function\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting memory-optimized GPT-2 training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m trained_gpt2, gpt2_memory_usage = train_model_memory_optimized(\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mgpt2_model\u001b[49m, train_dataloader, optimizer, scheduler, gpu_memory_tracker, EPOCHS\n\u001b[32m     61\u001b[39m )\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPT-2 training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'gpt2_model' is not defined"
     ]
    }
   ],
   "source": [
    "def train_model_memory_optimized(model, dataloader, optimizer, scheduler, memory_tracker, epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        memory_tracker.on_epoch_begin(epoch)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            memory_tracker.on_batch_begin(batch_idx)\n",
    "            \n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS  # Scale loss\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Only step optimizer every GRADIENT_ACCUMULATION_STEPS\n",
    "            if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "            progress_bar.set_postfix({'loss': loss.item() * GRADIENT_ACCUMULATION_STEPS})\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        memory_tracker.on_epoch_end(epoch)\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    return model, memory_tracker.memory_usage\n",
    "\n",
    "# Use the optimized training function\n",
    "print(\"Starting memory-optimized GPT-2 training...\")\n",
    "trained_gpt2, gpt2_memory_usage = train_model_memory_optimized(\n",
    "    gpt2_model, train_dataloader, optimizer, scheduler, gpu_memory_tracker, EPOCHS\n",
    ")\n",
    "print(\"GPT-2 training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN1eJIF_Txxx"
   },
   "source": [
    "As a final step, let's generate some text. The\n",
    "first call to `generate()` might be slow due to CUDA kernel initialization, but\n",
    "subsequent calls will be faster. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGza4-CKTxxx"
   },
   "outputs": [],
   "source": [
    "print(\"Generating text with fine-tuned GPT-2...\")\n",
    "generate_text(trained_gpt2, tokenizer, \"I like basketball\", max_length=MAX_GENERATION_LENGTH, device=device)\n",
    "generate_text(trained_gpt2, tokenizer, \"That Italian restaurant is\", max_length=MAX_GENERATION_LENGTH, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yee_V2ITxxx"
   },
   "source": [
    "## LoRA GPT-2\n",
    "\n",
    "In this section, we discuss the technical details of LoRA, build a LoRA GPT-2\n",
    "model using PEFT library, fine-tune it and generate text.\n",
    "\n",
    "### What exactly is LoRA?\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning technique for LLMs. It freezes the\n",
    "weights of the LLM, and injects trainable rank-decomposition matrices. Let's\n",
    "understand this more clearly.\n",
    "\n",
    "Assume we have an `n x n` pre-trained dense layer (or weight matrix), `W0`. We\n",
    "initialize two dense layers, `A` and `B`, of shapes `n x rank`, and `rank x n`,\n",
    "respectively. `rank` is much smaller than `n`. In the paper, values between 1\n",
    "and 4 are shown to work well.\n",
    "\n",
    "#### LoRA equation\n",
    "\n",
    "The original equation is `output = W0x + b0`, where `x` is the input, `W0` and\n",
    "`b0` are the weight matrix and bias terms of the original dense layer (frozen).\n",
    "The LoRA equation is: `output = W0x + b0 + BAx`, where `A` and `B` are the\n",
    "rank-decomposition matrices.\n",
    "\n",
    "LoRA is based on the idea that updates to the weights of the pre-trained\n",
    "language model have a low \"intrinsic rank\" since pre-trained language models are\n",
    "over-parametrized. Predictive performance of full fine-tuning can be replicated\n",
    "even by constraining `W0`'s updates to low-rank decomposition matrices.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.imgur.com/f4TFqMi.png\" alt=\"lora_diagram\" height=\"250\"/>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "#### Number of trainable parameters\n",
    "\n",
    "Let's do some quick math. Suppose `n` is 768, and `rank` is 4. `W0` has\n",
    "`768 x 768 = 589,824` parameters, whereas the LoRA layers, `A` and `B` together\n",
    "have `768 x 4 + 4 x 768 = 6,144` parameters. So, for the dense layer, we go from\n",
    "`589,824` trainable parameters to `6,144` trainable parameters!\n",
    "\n",
    "#### Why does LoRA reduce memory footprint?\n",
    "\n",
    "Even though the total number of parameters increase (since we are adding LoRA\n",
    "layers), the memory footprint reduces, because the number of trainable\n",
    "parameters reduces. Let's dive deeper into this.\n",
    "\n",
    "The memory usage of a model can be split into four parts:\n",
    "\n",
    "- Model memory: This is the memory required to store the model weights. This\n",
    "will be slightly higher for LoRA than GPT-2.\n",
    "- Forward pass memory: This mostly depends on batch size, sequence length, etc.\n",
    "We keep this constant for both models for a fair comparison.\n",
    "- Backward pass memory: This is the memory required to store the gradients.\n",
    "Note that the gradients are computed only for the trainable parameters.\n",
    "- Optimizer memory: This is the memory required to store the optimizer state.\n",
    "For example, the Adam optimizer stores the \"1st moment vectors\" and\n",
    "\"2nd moment vectors\" for the trainable parameters.\n",
    "\n",
    "Since, with LoRA, there is a huge reduction in the number of trainable\n",
    "parameters, the optimizer memory and the memory required to store the gradients\n",
    "for LoRA is much less than GPT-2. This is where most of the memory savings\n",
    "happen.\n",
    "\n",
    "#### Why is LoRA so popular?\n",
    "\n",
    "- Reduces GPU memory usage;\n",
    "- Faster training; and\n",
    "- No additional inference latency.\n",
    "\n",
    "### Create LoRA Model using PEFT\n",
    "\n",
    "We'll use Hugging Face's PEFT library to create a LoRA version of GPT-2.\n",
    "The PEFT library handles all the complexity of injecting LoRA adapters into\n",
    "the transformer layers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjkyd6HWTxxt"
   },
   "outputs": [],
   "source": [
    "# Clean up memory from previous model\n",
    "del trained_gpt2\n",
    "del optimizer\n",
    "del scheduler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory cleared. Loading fresh GPT-2 for LoRA...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30LpqmvQTxxw"
   },
   "source": [
    "### Configure and create LoRA model\n",
    "\n",
    "We'll configure LoRA to target the attention layers (query and value projections)\n",
    "of GPT-2, which is typically where LoRA shows the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjkyd6HWTxxt"
   },
   "outputs": [],
   "source": [
    "# Load a fresh GPT-2 model for LoRA\n",
    "base_model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=LORA_RANK,  # rank\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # target attention modules in GPT-2\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model = lora_model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34pXckGpTxxw"
   },
   "source": [
    "Let's do a forward pass to make sure we have a valid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVGCkn1GTxxw"
   },
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = tokenizer(\"LoRA is very useful for quick LLM finetuning\", return_tensors='pt')\n",
    "test_input = {k: v.to(device) for k, v in test_input.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model(**test_input)\n",
    "\n",
    "print(f\"Forward pass successful! Output shape: {outputs.logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40CZsC1mTxxw"
   },
   "source": [
    "### Fine-tune LoRA GPT-2\n",
    "\n",
    "Now that we have created the LoRA GPT-2 model, let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [],
   "source": [
    "# Initialize memory tracker for LoRA\n",
    "lora_memory_tracker = GPUMemoryTracker(\n",
    "    target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\n",
    "    print_stats=True,\n",
    ")\n",
    "\n",
    "# Get optimizer and scheduler for LoRA model\n",
    "lora_optimizer, lora_scheduler = get_optimizer_and_scheduler(lora_model, num_training_steps)\n",
    "\n",
    "print(\"Starting LoRA GPT-2 training...\")\n",
    "trained_lora_model, lora_memory_usage = train_model(\n",
    "    lora_model, train_dataloader, lora_optimizer, lora_scheduler, lora_memory_tracker, EPOCHS\n",
    ")\n",
    "print(\"LoRA GPT-2 training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42CZsC1mTxxw"
   },
   "source": [
    "### Compare memory usage and performance\n",
    "\n",
    "Let's compare the memory usage between standard fine-tuning and LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [],
   "source": [
    "# Plot memory usage comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    [\"Standard GPT-2\", \"LoRA GPT-2\"],\n",
    "    [max(gpt2_memory_usage), max(lora_memory_usage)],\n",
    "    color=[\"red\", \"blue\"],\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.ylabel(\"Peak GPU Memory Usage (GB)\")\n",
    "plt.title(\"GPU Memory Usage Comparison\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate([max(gpt2_memory_usage), max(lora_memory_usage)]):\n",
    "    plt.text(i, v + 0.1, f'{v:.2f}GB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate memory savings\n",
    "memory_savings = ((max(gpt2_memory_usage) - max(lora_memory_usage)) / max(gpt2_memory_usage)) * 100\n",
    "print(f\"\\nMemory savings with LoRA: {memory_savings:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46CZsC1mTxxw"
   },
   "source": [
    "### Generate text with LoRA model\n",
    "\n",
    "Let's generate text with our fine-tuned LoRA model. One of the advantages of LoRA\n",
    "is that there's no additional inference latency compared to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [],
   "source": [
    "print(\"Generating text with LoRA fine-tuned GPT-2...\")\n",
    "generate_text(trained_lora_model, tokenizer, \"I like basketball\", max_length=MAX_GENERATION_LENGTH, device=device)\n",
    "generate_text(trained_lora_model, tokenizer, \"That Italian restaurant is\", max_length=MAX_GENERATION_LENGTH, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48CZsC1mTxxw"
   },
   "source": [
    "### Save and Load LoRA Adapters\n",
    "\n",
    "One of the benefits of LoRA is that you can save only the adapter weights (which are much smaller)\n",
    "and load them on top of the base model when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "trained_lora_model.save_pretrained(\"./gpt2-lora-reddit\")\n",
    "print(\"LoRA adapters saved to ./gpt2-lora-reddit\")\n",
    "\n",
    "# Calculate adapter size\n",
    "import os\n",
    "adapter_size = sum(\n",
    "    os.path.getsize(os.path.join(\"./gpt2-lora-reddit\", f)) \n",
    "    for f in os.listdir(\"./gpt2-lora-reddit\") \n",
    "    if os.path.isfile(os.path.join(\"./gpt2-lora-reddit\", f))\n",
    ") / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "print(f\"LoRA adapter size: {adapter_size:.2f} MB\")\n",
    "print(\"Compare this to the full GPT-2 model which is ~500MB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50CZsC1mTxxw"
   },
   "source": [
    "### Load LoRA adapters (demonstration)\n",
    "\n",
    "Here's how you would load the LoRA adapters in a new session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model_for_inference = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_NAME)\n",
    "\n",
    "# Load LoRA adapters\n",
    "loaded_lora_model = PeftModel.from_pretrained(base_model_for_inference, \"./gpt2-lora-reddit\")\n",
    "loaded_lora_model = loaded_lora_model.to(device)\n",
    "\n",
    "print(\"LoRA model loaded successfully!\")\n",
    "\n",
    "# Test generation\n",
    "generate_text(loaded_lora_model, tokenizer, \"Today I learned that\", max_length=150, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52CZsC1mTxxw"
   },
   "source": [
    "## Summary and Comparison\n",
    "\n",
    "In this notebook, we've successfully demonstrated:\n",
    "\n",
    "1. **Standard Fine-tuning**: Traditional approach where all model parameters are updated\n",
    "2. **LoRA Fine-tuning**: Parameter-efficient approach using low-rank adaptation\n",
    "\n",
    "### Key Benefits of LoRA:\n",
    "\n",
    "- **Memory Efficiency**: Significantly reduced GPU memory usage during training\n",
    "- **Storage Efficiency**: LoRA adapters are much smaller than full model checkpoints\n",
    "- **Training Speed**: Faster training due to fewer parameters to update\n",
    "- **No Inference Overhead**: Same inference speed as the original model\n",
    "- **Modularity**: Easy to switch between different LoRA adapters for different tasks\n",
    "\n",
    "### When to use LoRA:\n",
    "\n",
    "- Limited GPU memory\n",
    "- Multiple task-specific adaptations needed\n",
    "- Quick experimentation and prototyping\n",
    "- Fine-tuning large models on consumer hardware\n",
    "\n",
    "This PyTorch implementation provides a complete, production-ready approach to parameter-efficient fine-tuning with LoRA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54CZsC1mTxxw"
   },
   "source": [
    "## Additional Utilities\n",
    "\n",
    "Here are some additional utility functions that might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO6rMQo9Txxx"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters in a model.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "def get_model_size_mb(model):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024 / 1024\n",
    "    return size_mb\n",
    "\n",
    "# Example usage\n",
    "print(\"Model Statistics:\")\n",
    "print(\"\\nLoRA Model:\")\n",
    "count_parameters(loaded_lora_model)\n",
    "print(f\"Model size: {get_model_size_mb(loaded_lora_model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56CZsC1mTxxw"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has successfully demonstrated how to implement parameter-efficient fine-tuning of GPT-2 using LoRA in PyTorch. The implementation is optimized for Linux systems with CUDA support and provides:\n",
    "\n",
    "- Complete PyTorch port from the original TensorFlow/Keras implementation\n",
    "- Modern best practices using Hugging Face Transformers and PEFT\n",
    "- Memory-efficient training with mixed precision\n",
    "- Comprehensive comparison between standard and LoRA fine-tuning\n",
    "- Practical utilities for model analysis and deployment\n",
    "\n",
    "LoRA continues to be one of the most effective parameter-efficient fine-tuning techniques, enabling efficient adaptation of large language models even on resource-constrained hardware.\n",
    "\n",
    "Happy fine-tuning! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "parameter_efficient_finetuning_of_gpt2_with_lora_pytorch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
